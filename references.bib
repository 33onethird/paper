% Encoding: UTF-8
@inproceedings{drebin,
	author = {Daniel {Arp} and Michael {Spreitzenbarth} and Malte {Huebner} and Hugo {Gascon} and Konrad {Rieck}},
	title = {Drebin: {Efficient} and {Explainable} {Detection} of {Android} {Malware} in {Your} {Pocket}},
	year = 2014,
	month = feb,
	booktitle={21th Annual Network and Distributed System Security Symposium (NDSS)}
}

@Misc{staticdynamic,
  author       = {Amir Ghahrai},
  title        = {Static Analysis vs Dynamic Analysis in Software Testing},
  howpublished = {https://www.testingexcellence.com/static-analysis-vs-dynamic-analysis-software-testing/},
  month        = may,
  year         = {2016},
  note         = {Accessed 10.1.2018},
  timestamp    = {2018-01-10},
}

@book{cristianini2000introduction,
	title={An introduction to support vector machines and other kernel-based learning methods},
	author={Cristianini, Nello and Shawe-Taylor, John},
	year={2000},
	publisher={Cambridge university press}
}
@ARTICLE{7917369, 
	author={A. Demontis and M. Melis and B. Biggio and D. Maiorca and D. Arp and K. Rieck and I. Corona and G. Giacinto and F. Roli}, 
	journal={IEEE Transactions on Dependable and Secure Computing}, 
	title={Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection}, 
	year={2017}, 
	volume={PP}, 
	number={99}, 
	pages={1-1}, 
	keywords={Algorithm design and analysis;Androids;Feature extraction;Humanoid robots;Malware;Security;Tools;Android Malware Detection;Computer Security;Secure Machine Learning;Static Analysis}, 
	doi={10.1109/TDSC.2017.2700270}, 
	ISSN={1545-5971}, 
	month={},}
@article{scikit-learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}
@book{Manning:2008:IIR:1394399,
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
	title = {Introduction to Information Retrieval},
	year = {2008},
	isbn = {0521865719, 9780521865715},
	publisher = {Cambridge University Press},
	address = {New York, NY, USA},
}
@article{10.2307/2983890,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2983890},
	abstract = {A sequence of 0's and 1's is observed and it is suspected that the chance that a particular trial is a 1 depends on the value of one or more independent variables. Tests and estimates for such situations are considered, dealing first with problems in which the independent variable is preassigned and then with independent variables that are functions of the sequence. There is a considerable amount of earlier work, which is reviewed.},
	author = {D. R. Cox},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {2},
	pages = {215-242},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {The Regression Analysis of Binary Sequences},
	volume = {20},
	year = {1958}
}
@INPROCEEDINGS{598994, 
	author={Tin Kam Ho}, 
	booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, 
	title={Random decision forests}, 
	year={1995}, 
	volume={1}, 
	number={}, 
	pages={278-282 vol.1}, 
	keywords={decision theory;handwriting recognition;optical character recognition;complexity;decision trees;generalization accuracy;handwritten digits;random decision forests;stochastic modeling;suboptimal accuracy;tree-based classifiers;Classification tree analysis;Decision trees;Handwriting recognition;Hidden Markov models;Multilayer perceptrons;Optimization methods;Stochastic processes;Testing;Tin;Training data}, 
	doi={10.1109/ICDAR.1995.598994}, 
	ISSN={}, 
	month={Aug},}
@article{rosenblatt1958,
	title={The perceptron: a probabilistic model for information storage and organization in the brain.},
	author={Rosenblatt, Frank},
	journal={Psychological review},
	volume={65},
	number={6},
	pages={386},
	year={1958},
	publisher={American Psychological Association}
}
@Article{Fukushima1980,
	author="Fukushima, Kunihiko",
	title="Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
	journal="Biological Cybernetics",
	year="1980",
	month="Apr",
	day="01",
	volume="36",
	number="4",
	pages="193--202",
	abstract="A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
	issn="1432-0770",
	doi="10.1007/BF00344251",
	url="https://doi.org/10.1007/BF00344251"
}
@article{doi:10.1162/neco.1997.9.8.1735,
	author = { Sepp Hochreiter  and  JÃ¼rgen Schmidhuber },
	title = {Long Short-Term Memory},
	journal = {Neural Computation},
	volume = {9},
	number = {8},
	pages = {1735-1780},
	year = {1997},
	doi = {10.1162/neco.1997.9.8.1735},
	
	URL = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	
	},
	eprint = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	
	}
	,
	abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}
